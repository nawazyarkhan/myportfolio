{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-my-portfolio-page","title":"Welcome to My Portfolio Page","text":"<p>For full list of projects and repositories visit: My GitHub Profile.</p>"},{"location":"#my-social-media-links","title":"My Social Media Links","text":"<ul> <li>LinkedIn </li> <li>facebook</li> <li>twitter/x</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<ul> <li>My Projects related to Python, ML/AI are under Projects</li> </ul>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-me","title":"About Me","text":"<p>Welcome to my world of data and innovation! \ud83d\ude80 I am a passionate Data Scientist/Data Engineer/ML Engineer </p>"},{"location":"about/#my-journey","title":"My Journey \ud83c\udf0d","text":"<p>I have an extensive experience of over 25 years in the field of datawarehousing/ data analytics/ machine learning/ data science. Insatiable intellectual curiosity and ability to mine hidden gems located within large sets of structured, semi-structured, and raw data.</p> <p>Have an overall 25 years of experience working with Teradata. Extensive hands-on experience Teradata performance and Query optimization, Teradata ecosystem components like data mover, Query Grid, Teradata data migration engagements, teradataml/ ClearScape. I have been in providing Teradata backup solutions to Teradata customers. Also, extensive experience on various Unix &amp; Linux platforms running Teradata systems and other supporting systems as well as extensive experience on shell scripting.  I\u2019m also actively building my skillset on various ML projects using regression, classification &amp; deep learning using scikit-learn, TensorFlow to further enhance my skills in ML side of thing.  Hands on experience building RAG &amp; LLM based apps using python streamlit, Django, various Ollama models &amp; hugging face models.</p>"},{"location":"blogs/","title":"Blogs","text":"<p>Coming soon...Please stay tuned!</p>"},{"location":"blogs/#posts","title":"Posts","text":"<ul> <li>A Practical Guide to Large Language Models (LLMs) \u2014 Strengths &amp; Best Uses</li> </ul> <p>More posts will be added here as I publish them.</p>"},{"location":"projects/","title":"Here are some of the projects done at UOB Data analytics bootcamp &amp; DSAAMP course at condanics:","text":""},{"location":"projects/#personal-portfolio-using-mkdocs","title":"Personal portfolio using mkdocs","text":"<p>Description: Created my portfolio website using python framework mkdocs. project details/code can be found in my github repository https://github.com/nawazyarkhan/myportfolio</p>"},{"location":"projects/#local-llm-to-help-answer-user-questions-from-provided-textpdfdoc-files","title":"Local LLM to help answer user questions from provided text/PDF/doc files","text":"<p>Description:   Created RAG based app using Ollama model llama3 that takes test/PDF/doc file as an input   using streamlit GUI.   Details can be found under github repository https://github.com/nawazyarkhan/localRAG_AI</p>"},{"location":"projects/#classification-using-deeplearning","title":"Classification using deeplearning","text":"<p>Description: The non-profit foundation Alphabet Soup wants to create an algorithm to predict whether or not applicants for funding will be successful. With your knowledge of machine learning and neural networks, use the features in the provided dataset to create a binary classifier that is capable of predicting whether applicants will be successful if funded by Alphabet Soup. From Alphabet Soup\u2019s business team, you have received a CSV containing more than 34,000 organizations that have received funding from Alphabet Soup over the years. Details/code can be found under github repository https://github.com/nawazyarkhan/DeepLearningHomeWork</p>"},{"location":"projects/#credit-risk-evaluator-using-logistic-regression-random-forest","title":"Credit risk evaluator using logistic regression &amp; random forest","text":"<p>Description: This project involves analyzing and predicting a provided dataset of loan applicants to evaluate &amp; predict the risk involved in loan approval. Details/code can be found under my github repository https://github.com/nawazyarkhan/SupervisedMachineLearningHomework</p>"},{"location":"resume/","title":"\ud83d\udcc4 My Resume","text":""},{"location":"resume/#about-me","title":"\ud83d\udc68\u200d\ud83d\udcbb About Me","text":"<p>Hi, I'm Nawaz Y. khan! A passionate Data Analyst/Data Scientist/ML Engineer with expertise in turning data into actionable insights. I thrive on solving complex problems and delivering data-driven solutions.</p> <ul> <li>\ud83d\udccd Location: Birmingham, UK</li> <li>\ud83d\udce7 Email: nawazyarkhan@gmail.com</li> <li>\ud83c\udf10 Portfolio: github.io/nawazyarkhan</li> <li>\ud83d\udcbc LinkedIn: www.linkedIn.com/nawaz-yar-khan</li> </ul>"},{"location":"resume/#education","title":"\ud83c\udf93 Education","text":"<ul> <li> <p>[B.Sc Electrical Engineering]     \ud83d\udccd University of Engineering &amp; Technology Lahore, PK 1998  </p> <ul> <li>Relevant Courses: Electrical &amp; communications</li> </ul> </li> <li> <p>[PostGrad Diploma in Data Analytics]     \ud83d\udccd University of Birmingham, 2021  </p> <ul> <li>Relevant Courses: Data analytics/ ML</li> </ul> </li> </ul>"},{"location":"resume/#work-experience","title":"\ud83d\udcbc Work Experience","text":""},{"location":"resume/#ps-consultant","title":"PS Consultant","text":"<p>\ud83d\udccd Teradata UK Pvt Limited, Jun2022 - Present   \ud83d\udd39  LLOYDS data migration Project \u2013 I have been actively involved with the data migration project using various data migration related tasks. I have been tasked to optimize queries involved in using big tables where I have managed to optimize by changing the table DDLs and optimizing queries which helped to reduce the CPU &amp; IO.                                                                                  </p> <p>\ud83d\udd39  HSBC Cloud Project \u2013 I have been tasked to configure the Query Grid in the cloud. HSBC has got multiple systems both on-prem &amp; cloud making it one of the most complex configurations and I have been able to successfully configure &amp; test the QueryGrid. Also, I have been given the task to configure the data mover across 5 systems in cloud. </p> <p>\ud83d\udd39  BOI Security hardening project \u2013 I have been tasked for OS security hardening which is one of the Teradata PS engagements and have successfully completed it across their PROD &amp; DR systems. </p> <p>\ud83d\udd39  LLOYDS Data Mover &amp; Viewpoint DR test project - I have managed a project for a failover test of viewpoint &amp; data mover to simulate a DR exercise which I have completed smoothly. </p> <p>\ud83d\udd39  LLOYDS security hardening project \u2013 I have been involved actively in addressing various security issues identified in the security penetration test across all their systems estate.</p> <p>\ud83d\udd39  PTSB Datamover framework setup project - Worked on Datamover setup project in PTSB to help automate the datamover jobs to sync data from production site to DR site.</p> <p>\ud83d\udd39  Admiral Data Migration project \u2013 I have been involved with the data migration project to cloud system using data mover. Also, have successfully setup the LDAP on cloud system.</p>"},{"location":"resume/#data-migrations-technical-team-lead","title":"Data Migrations technical Team lead","text":"<p>\ud83d\udccd Teradata UK Pvt Limited      Dec2020 \u2013 Jun2022    \ud83d\udd39 I have been promoted as team lead of the team providing consultancy/pre-sales support and  implementations of data migrations. As a team lead, I\u2019m acting as technical mentor for the team, doing the project assignments as well as handling complex migration projects in the EMEA region. Currently focused on doing migrations from on-prem to cloud systems which is in line with focus of moving to cloud. </p>"},{"location":"resume/#data-migration-bar-consultant","title":"Data Migration/ BAR consultant","text":"<p>\ud83d\udccd Teradata UK Pvt Limited      Aug2018 - Dec2020    \ud83d\udd39 I have been promoted as team lead of the team providing consultancy/pre-sales support and  implementations of data migrations. As a team lead, I\u2019m acting as technical mentor for the team, doing the project assignments as well as handling complex migration projects in the EMEA region. Currently focused on doing migrations from on-prem to cloud systems which is in line with focus of moving to cloud. </p>"},{"location":"resume/#change-control-specialist","title":"Change Control Specialist","text":"<p>\ud83d\udccd Teradata UK Pvt Limited      Jul2016 - Aug2018    \ud83d\udd39 I've worked as part of the regional International Change control team as a change control specialist who is responsible for all major/minor Teradata upgrades/configuration changes/expansion etc in the International Region (UK/Europe, Middle East/Africa/Asia Pacific etc) . I have also been involved with Hadoop upgrades. </p>"},{"location":"resume/#senior-systems-engineer-technical-account-manager","title":"Senior Systems Engineer / Technical Account Manager","text":"<p>\ud83d\udccd Teradata UK Pvt Limited      Mar2011 - Jul2016   \ud83d\udd39 I have been involved with Minor, Major Teradata upgrades, and Teradata hardware    expansions/merge activities. I have an extensive hardware &amp; software support experience on  66xx, 67xx, 27xx, 2800, 52xx, 53xx, 54xx, 55xx, 56xx, 66xx, 26xx, 25xx, etc. &amp; various LSI  platforms 684x.Also, have an extensive hands-on experience on ECO systems (TMSM, Viewpoint and Datamover) upgrades.</p>"},{"location":"resume/#senior-systems-engineer-technical-team-lead","title":"Senior Systems Engineer / Technical Team Lead","text":"<p>\ud83d\udccd NCR/Teradata Corporation, Pakistan   Sep2000 - Mar2011   \ud83d\udd39I have worked as \u201cSenior Systems Engineer/Technical Team lead\u201d based in Lahore providing onsite support to critical customers as well as remote support/2nd level of support to the local team of CSRs on various support calls. During tenure of last 11 years, I have done many critical installations/expansions/upgrades of DWH systems at various customer sites like Mobilink DWH, Bank Alfalah DWH, Ufone DWH, ACBL DWH, ITDPunjab DWH, RBL DWH, ZTE-PTCL DWH, Telenor DWH, &amp; Telenor and Bahria Town Call Centers. I have also been involved with the Netvault/Netbackup installation &amp; configuration, ACSLS, Storagetek tape libraries like L180, SL500 at various sites in Pakistan. Have successfully done the netvault Installation, SAN switch configuration at one of the biggest customers in Pakistan (Mobilink). Have also done a data migration using tapes in a netvault environment in Bank Alfalah Karachi Pakistan.  </p>"},{"location":"resume/#skills","title":"\ud83d\udee0\ufe0f Skills","text":"<ul> <li>Exploratory data analysis (EDA)</li> <li>Python (Dataframes, NumPy) / Machine learning / deep learning (scikit learn, TensorFlow) </li> <li>Visualization tools  - Tableau, Matplotlib, Seaborn, Plotly </li> <li>MLOps </li> <li>Web scraping </li> <li>LLM &amp; RAG based models &amp; building AI agents</li> <li>Django framework</li> <li>SUSE Linux/shell scripting, OS Security hardening</li> <li>Teradata BAR solution (Netbackup/DSA)</li> <li>Teradata Query &amp; database performance optimization</li> <li>Teradata QG &amp; Data mover configuration </li> <li>DTU &amp; NPARC DMS tool (TD internal developed for data migrations), NetBackup, Netvault</li> </ul>"},{"location":"resume/#certifications-professional-courses","title":"\ud83c\udfc6 Certifications &amp; Professional Courses","text":"<ul> <li>Supervised Machine Learning: Regression and Classification from Coursera</li> <li>Advanced Learning Algorithms from Coursera</li> <li>Unsupervised Learning, Recommenders, Reinforcement Learning from Coursera</li> <li>Data Science and AI Mentorship Program (DSAAMP) from codanics.com</li> <li>Data analytics boot camp University of Birmingham</li> <li>Vantage Associate 2.3 Certified</li> <li>Vantage cloud lake certified associate</li> <li>Vantage Administration Certified </li> <li>Vantage Data engineering certified </li> <li>Teradata\u2019s Internal Architecture Foundation program (Mar\u2019 24 \u2013 Jan\u2019 25) \u2013 I have been selected across the globe along with a group of selected people who are being trained on architecture foundation program involving developing skills on new technologies and various hands-on activities &amp; presentations to help develop the skills for a prospect solution architect. </li> <li>Teradata Certified Master </li> <li>Teradata Certified support associate</li> <li>Amazon AWS certified solution architect \u2013 Associate </li> </ul>"},{"location":"resume/#interests","title":"\ud83c\udf1f Interests","text":"<ul> <li>\ud83d\udcc8 Data Storytelling  </li> <li>\ud83e\udd16 Artificial Intelligence  </li> <li>\ud83c\udf0d Open Source Contributions  </li> <li>\ud83d\udcda Continuous Learning  </li> </ul>"},{"location":"resume/#contact-me","title":"\ud83d\udcec Contact Me","text":"<p>Feel free to reach out via email or connect with me on LinkedIn!</p>"},{"location":"skills/","title":"My Skillset","text":"<ul> <li>Exploratory data analysis (EDA)</li> <li>Python (Dataframes, NumPy) / Machine learning / deep learning (scikit learn, TensorFlow) </li> <li>Visualization tools  - Tableau, Matplotlib, Seaborn, Plotly </li> <li>MLOps </li> <li>Web scraping </li> <li>LLM &amp; RAG based models &amp; building AI agents</li> <li>Django framework</li> <li>SUSE Linux/shell scripting, OS Security hardening</li> <li>Teradata BAR solution (Netbackup/DSA)</li> <li>Teradata Query &amp; database performance optimization</li> <li>Teradata QG &amp; Data mover configuration </li> <li>DTU &amp; NPARC DMS tool (TD internal developed for data migrations), NetBackup, Netvault</li> </ul>"},{"location":"blogs/llm-guide/","title":"A Practical Guide to Large Language Models (LLMs) \u2014 Strengths &amp; Best Uses","text":"<p>TL;DR: This post compares major LLM families (OpenAI GPT, Anthropic Claude, Google Gemini/PaLM, Meta Llama, Mistral, Falcon/MPT, and other alternatives), explains their strengths and weaknesses, and recommends which to choose for chat assistants, code, RAG, on-device inference, and cost-sensitive workflows.</p>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#why-this-guide","title":"Why this guide","text":"<ul> <li>Inputs: You need to pick an LLM for a concrete task (chatbot, summarization, code assistant, RAG, or self-hosting).</li> <li>Outputs: Short list of recommended models, strengths/weaknesses, example prompts, and a quick decision checklist.</li> </ul>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#quick-model-by-model-summary","title":"Quick model-by-model summary","text":"<ul> <li>OpenAI \u2014 GPT-4 / GPT-3.5</li> <li>Strengths: Best-in-class instruction following, reasoning, code generation, rich SDKs and plugin ecosystems.</li> <li>Weaknesses: Cost and vendor lock-in; data residency concerns for sensitive data.</li> <li> <p>Best uses: Production chat assistants, code generation, high-quality summarization, RAG generation.</p> </li> <li> <p>Anthropic \u2014 Claude</p> </li> <li>Strengths: Safety-focused, strong instruction following in multi-turn dialogues.</li> <li>Weaknesses: Conservative responses at times, smaller ecosystem vs OpenAI.</li> <li> <p>Best uses: Customer support assistants, internal knowledge-base chatbots where safety is essential.</p> </li> <li> <p>Google \u2014 PaLM / Gemini</p> </li> <li>Strengths: Multimodal capabilities and tight Google Cloud integration.</li> <li>Weaknesses: Ecosystem lock-in; pricing varies.</li> <li> <p>Best uses: Multimodal apps, enterprises on Google Cloud, search-enhanced tasks.</p> </li> <li> <p>Meta \u2014 Llama (2/3)</p> </li> <li>Strengths: Open weights, easy fine-tuning and self-hosting, strong community tooling.</li> <li>Weaknesses: May trail top proprietary models on some benchmarks.</li> <li> <p>Best uses: Self-hosted assistants, fine-tuning on private corpora, privacy-sensitive deployments.</p> </li> <li> <p>Mistral</p> </li> <li>Strengths: Strong open-source performance, efficiency-focused models.</li> <li>Weaknesses: Newer ecosystem; tooling still maturing.</li> <li> <p>Best uses: Cost-effective inference, on-prem quantized deployments.</p> </li> <li> <p>Falcon / MPT</p> </li> <li>Strengths: Open models suitable for experimentation and domain fine-tuning.</li> <li>Weaknesses: Mixed benchmark results; may need engineering effort for production.</li> <li> <p>Best uses: Research, domain-specific fine-tuning, cost-conscious self-hosting.</p> </li> <li> <p>Cohere, Aleph Alpha, and other providers</p> </li> <li>Strengths: Alternative APIs, multilingual support, embeddings, and classification features.</li> <li>Best uses: Embeddings, classification, and non-US-centric multilingual tasks.</li> </ul>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#small-comparison-table","title":"Small comparison table","text":"Model / Provider Strengths Best uses Latency Cost Fine-tuning Self-hostable Notes OpenAI GPT-4 Top accuracy, tooling, plugins Chat, code, RAG gen Medium High Limited (OpenAI fine-tune options) No (API) Excellent for production when budget allows Anthropic Claude Safety, long-dialog handling Support bots, safe assistants Medium Medium-High Vendor fine-tuning No (API) Strong safety posture Google Gemini/PaLM Multimodal, GCP integration Multimodal apps, enterprise Medium High API-based fine-tuning No (API) Great for image+text tasks Meta Llama 2/3 Open weights, self-hosting Fine-tuning, on-prem assistants Variable Low (self-host infra cost) Yes (community tools) Yes Good balance for privacy-focused apps Mistral Efficient, strong open performance Cost-effective inference Low-Medium Low Yes Yes Fast-moving open-source option Falcon / MPT Research &amp; domain fine-tuning Experimentation, edge cases Variable Low Yes Yes Useful for custom workloads Cohere / Aleph Alpha Embeddings &amp; multilingual Embeddings, classification Low-Medium Medium Yes No/limited Provider alternatives with unique strengths <p>Notes: \"Latency\" and \"Cost\" are generalizations \u2014 actual values depend heavily on hosting, model size, and usage pattern.</p>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#choosing-by-task-concise","title":"Choosing by task (concise)","text":"<ul> <li>Chat &amp; reasoning: GPT-4 or Claude 3; self-hosted option \u2014 Llama 3 with RAG.</li> <li>Code assistants: GPT-4 (best), or fine-tuned Llama/Mistral for on-prem.</li> <li>RAG &amp; long-docs: Use models with long-context support or combine embeddings + RAG with a strong generator (GPT, Claude).</li> <li>Cost-sensitive bulk generation: GPT-3.5 or quantized Llama/Mistral/Falcon.</li> <li>Multimodal: Gemini or OpenAI multimodal variants.</li> <li>On-device/privacy: Quantized Llama, Mistral, Falcon with llama.cpp/ggml or vLLM.</li> </ul>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#quick-decision-checklist","title":"Quick decision checklist","text":"<ol> <li>Data sensitivity: must data stay on-prem? If yes \u2192 self-host (Llama/Mistral/Falcon).</li> <li>Budget per request: high \u2192 GPT-4; low \u2192 GPT-3.5 or quantized open models.</li> <li>Latency requirements: low-latency \u2192 smaller/quantized models near users.</li> <li>Integration needs: plugins/tools \u2192 OpenAI has richest ecosystem.</li> <li>Multimodal requirements: Gemini or OpenAI multimodal models.</li> <li>Need to fine-tune: prefer open weights (Llama family, Mistral) or providers with fine-tuning APIs.</li> </ol>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#practical-prompts-examples","title":"Practical prompts (examples)","text":"<ul> <li>Summarization: \"Summarize the following text in 5 bullets focusing on action items and risks: &lt;&gt;\" <li>Classification (few-shot): \"Label the sentiment of these examples (POS/NEG/NEUTRAL). Return JSON array of labels.\"</li> <li>RAG assistant: \"Given the retrieved documents below and the user question, provide a concise answer and cite document IDs used.\"</li> <li>Code explanation: \"Explain the following Python function in simple terms and suggest two tests for edge cases: &lt;<code>&gt;\"","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#operational-checklist-engineering","title":"Operational checklist (engineering)","text":"<ul> <li>Start with a small POC to measure latency, cost, and hallucination rates.</li> <li>For self-hosting, test quantized inference on your target hardware before committing to a model.</li> <li>Add monitoring for hallucination, latency, and user satisfaction.</li> </ul>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#caveats","title":"Caveats","text":"<ul> <li>Hallucination: All LLMs can hallucinate \u2014 use RAG/verification for critical outputs.</li> <li>Safety: Add guardrails and human review for high-risk outputs.</li> <li>Licensing: Check open-model licenses before commercial use.</li> </ul>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"blogs/llm-guide/#suggested-next-steps","title":"Suggested next steps","text":"<ul> <li>Run a quick benchmark with 100 representative requests for cost/latency estimates.</li> <li>If you want, I can add a short comparison CSV or a small chart to visualize cost vs accuracy trade-offs.</li> </ul> <p>If you'd like, I can also add a link to this new post from your main <code>blogs.md</code> or create a landing index under <code>docs/blogs/index.md</code>.</p>","tags":["LLM","GPT","Claude","Llama","Mistral","RAG","Fine-tuning"]},{"location":"projects/project_1/","title":"Project 1","text":""},{"location":"projects/project_1/#mkdocs-to-build-the-portfolio","title":"mkdocs to build the portfolio","text":"<p>Details how built </p>"}]}